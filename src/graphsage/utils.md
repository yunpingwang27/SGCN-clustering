$B_i(2)$由两部分组成：
- 与$B_i(1)$中的节点有正边连接的节点；
- 与$U_i(1)$中的节点有负边连接的节点。

类似地，我们可以得到，对$U_i(2)$，我们要求与节点$i$之间是以一个正边一个负边（无论顺序）相连，也就是$Bi(1)$的负邻居节点，与$U_i(1)$的正邻居节点的并集。



于是，当我们已知$B_i(l)$和$U_i(l)$，这时候$B_i(l+1)$应该等于与节点$i$的长度为$l$的平衡路径的节点集合，也就是$B_i(l)$里节点的正邻居节点，再加上与节点$i$以长度为$l$的非平衡路径相连的节点集合，也就是$U_i(l)$这些点的负邻居节点。

类似地，我们可以推出，$U_i(l+1)$，是与$U_i(l)$中节点正连接的节点集合，与和$B_i(l)$中节点负连接的节点集合的并集。

我们认为节点$i$的平衡集中的节点，可以认为是$i$的"朋友"；而$i$的不平衡集中的节点，我们推测是$i$的"敌人"。

$l=1,2,3，……，L$表示与$u_i$相连的路径的长度，表示在SGCN的第几层。

我们可以看到节点i的两个正邻居，也就是Bi(1)的信息，将通过使用“朋友”聚合器B(1)被合并到第一层的“朋友”的嵌入表示中。也就是说，对于层数l而言，B(l)表示的是聚合Bi(l)中的元素的信息。
然后，通过在我们的GCN中使用第二层，我们对两阶邻居节点进行合并。
具体的合并呢，我们是采取了第二组聚合器B(2)和U(2)，分别将把Bi(2)和Ui(2)的信息聚合起来，以便于传播。B(2)聚合器中，所聚合的节点要么是与ui有两个正连接，要么是两个负连接的，也就是Bi(2)中的节点；类似地，U(2)聚合器，聚合的是Ui(2)中的节点的信息。


$𝑙$表征与节点$𝑖$相连的路径的长度，也表示在SGCN的第几层。
$𝐵(𝑙)$节点$𝑖$第$𝑙$层的“朋友”聚合器。
$𝑈(𝑙)$节点$i$第$𝑙$层的“敌人”聚合器。
$B(l)$的信息来源：$B_i(l)$与节点$i$
$U(l)$的信息来源：$U_i(l)$与节点$i$

$N_i^+$指的是节点i的正邻居节点的集合；
$N_i^-$指的是节点i的负邻居节点的集合。

我们先来明确一下最后的输出$z_i$，其中$L$指的是SGCN的总层数。

目前明确的输入：

$G = (U,E^+,E^-)$;初始的节点表示：$\{x_i,\forall u_i \in U\}$, $d^{in}$维;激活函数$\sigma$


输出：

$$z_i \leftarrow [h_i^{B(L)},h_i^{U(L)}]$$

这是由节点$i$的朋友嵌入表示与敌人嵌入表示连缀而成的向量，其维度同样是由我们事先指定的，记为$2d^{out}$。

类似一般的GCN，我们在开始的时候指定$h_i(0)\in R^{d^{in}}$为节点$i$初始的特征,也即:
$$h_i^{(0)}\leftarrow x_i,\forall i \in U $$

来表示初始输入的节点$i$的特征。

节点$i$第$l$层的“朋友”聚合信息来源：
- 节点$i$
- 节点$i$的$l$阶平衡集：$B_i(l)$，它包括两部分：
  - 节点$i$的正邻居节点的$l-1$阶平衡集
  - 节点$i$的负邻居节点的$l-1$阶非平衡集。
以上节点的$l-1$层嵌入表示信息。

我们构造了一小批量的节点样本，然后构造了一个集合$M$，其中包含形式为$(u_i，u_j，s)$的元素，表示节点对$(u_i，u_j)$以及$s∈\{+，−，？\}$，也就是用于表示节点对之间是否存在正、负或无连接。

$$w_s = \frac{1}{n_s}$$

- $λ$为超参数，用于平衡该项在损失函数中所占比重；
- $M_{(+,?)}$表征的是正边相连的节点对集合中，对每一个节点对$(u_i,u_j)$，我们引入一个未与$u_i$相连的节点$u_k$(每次迭代都选取不同的$u_k$)，$M_{(-,?)}$与之同理；
- $||z_i-z_j||^2_2$表示的是节点$u_i$与$u_j$之间欧几里得距离的平方。
  
  
DeepWalk是一种基于随机游走的网络嵌入方法，用于将网络中的节点表示为低维向量。它的损失函数采用了负采样（negative sampling）的思想，可以简化计算。具体地，DeepWalk的损失函数可以表示为：$$\mathcal{L} = -\sum_{u \in V} \sum_{v \in \mathcal{N}(u)} \log\sigma(\mathbf{u}^\top\mathbf{v}) - \sum_{u \in V} k_u \sum_{v \in V \setminus \mathcal{N}(u)} \log\sigma(-\mathbf{u}^\top\mathbf{v})$$

其中，$V$ 是网络中所有节点的集合，$\mathcal{N}(u)$ 表示节点 $u$ 的所有邻居节点集合，$\mathbf{u}$ 和 $\mathbf{v}$ 分别表示节点 $u$ 和节点 $v$ 的低维向量表示，$\sigma(x)$ 表示 sigmoid 函数，$k_u$ 表示节点 $u$ 的采样数量，通常为一个常数。

第一项是正样本的损失，表示节点 $u$ 与其邻居节点 $v$ 的相似度越高，损失越小。第二项是负样本的损失，表示节点 $u$ 与非邻居节点 $v$ 的相似度越低，损失越小。整个损失函数的目标是最小化所有节点的损失之和。


DeepWalk是一种基于随机游走的图嵌入方法，它将节点表示为低维向量，以便在机器学习任务中使用。DeepWalk使用的损失函数是负对数似然损失函数，可以用以下数学公式表示：$L = -\frac{1}{|\mathcal{V}|}\sum_{v\in \mathcal{V}}\sum_{u\in \mathcal{N}(v)}\log p(u|v)$

其中，$\mathcal{V}$是图中所有节点的集合，$\mathcal{N}(v)$是节点$v$的邻居节点的集合，$p(u|v)$是从节点$v$出发，到达节点$u$的概率。DeepWalk将$p(u|v)$建模为softmax函数：

$p(u|v) = \frac{\exp(\mathbf{v}\cdot\mathbf{u})}{\sum_{i\in \mathcal{V}}\exp(\mathbf{v}\cdot\mathbf{i})}$

其中，$\mathbf{v}$表示节点$v$的向量表示，$\mathbf{u}$表示节点$u$的向量表示。因此，DeepWalk的损失函数可以进一步表示为：

$L = -\frac{1}{|\mathcal{V}|}\sum_{v\in \mathcal{V}}\sum_{u\in \mathcal{N}(v)}\log\frac{\exp(\mathbf{v}\cdot\mathbf{u})}{\sum_{i\in \mathcal{V}}\exp(\mathbf{v}\cdot\mathbf{i})}$

$L = -\frac{1}{|\mathcal{V}|}\sum_{v\in \mathcal{V}}\sum_{u\in \mathcal{N^+}(v)}\log\frac{\exp(\mathbf{v}\cdot\mathbf{u})}{\sum_{i\in \mathcal{V}}\exp(\mathbf{v}\cdot\mathbf{i})}+\frac{1}{|\mathcal{V}|}\sum_{v\in \mathcal{V}}\sum_{u\in \mathcal{N^-}(v)}\log\frac{\exp(\mathbf{v}\cdot\mathbf{u})}{\sum_{i\in \mathcal{V}}\exp(\mathbf{v}\cdot\mathbf{i})}$

这个损失函数的意义是，对于每个节点$v$，DeepWalk希望将$v$的向量表示$\mathbf{v}$与它的邻居节点的向量表示$\mathbf{u}$之间的点积最大化，同时将$v$的向量表示$\mathbf{v}$与所有节点的向量表示$\mathbf{i}$之间的点积最小化。